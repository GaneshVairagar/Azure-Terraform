{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "provenance": []
    },
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3"
    },
    "language_info": {
      "name": "python"
    }
  },
  "cells": [
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "gpKSZX5Iq1A6"
      },
      "outputs": [],
      "source": [
        "import matplotlib.pyplot as plt\n",
        "import seaborn as sns\n",
        "import matplotlib as mpl\n",
        "import matplotlib.pylab as pylab\n",
        "import numpy as np\n",
        "%matplotlib inline\n",
        "\n",
        "#Data Prepration\n",
        "import re\n",
        "\n",
        "sentences = \"\"\"We are about to study the idea of a computational process.\n",
        "Computational processes are abstract beings that inhabit computers.\n",
        "As they evolve, processes manipulate other abstract things called data.\n",
        "The evolution of a process is directed by a pattern of rules\n",
        "called a program. People create programs to direct processes. In effect,\n",
        "we conjure the spirits of the computer with our spells.\"\"\"\n",
        "\n",
        "# remove special characters\n",
        "sentences = re.sub('[^A-Za-z0-9]+', ' ', sentences)\n",
        "\n",
        "# remove 1 letter words\n",
        "sentences = re.sub(r'(?:^| )\\w(?:$| )', ' ', sentences).strip()\n",
        "\n",
        "# lower all characters\n",
        "sentences = sentences.lower()\n",
        "\n",
        "words = sentences.split()\n",
        "vocab = set(words)\n",
        "\n",
        "vocab_size = len(vocab)\n",
        "embed_dim = 10\n",
        "context_size = 2\n",
        "\n",
        "word_to_ix = {word: i for i, word in enumerate(vocab)}\n",
        "ix_to_word = {i: word for i, word in enumerate(vocab)}\n",
        "\n",
        "# data - [(context), target]\n",
        "\n",
        "data = []\n",
        "for i in range(2, len(words) - 2):\n",
        "    context = [words[i - 2], words[i - 1], words[i + 1], words[i + 2]]\n",
        "    target = words[i]\n",
        "    data.append((context, target))\n",
        "print(data[:5])\n",
        "Output:\n",
        "[(['we', 'are', 'to', 'study'], 'about'), (['are', 'about', 'study', 'the'], 'to'), (['about', 'to', 'the', 'idea'], 'study'), (['to', 'study', 'idea', 'of'], 'the'), (['study', 'the', 'of', 'computational'], 'idea')]\n",
        "\n",
        "embeddings =  np.random.random_sample((vocab_size, embed_dim))\n",
        "\n",
        "def linear(m, theta):\n",
        "    w = theta\n",
        "    return m.dot(w)\n",
        "\n",
        "def log_softmax(x):\n",
        "    e_x = np.exp(x - np.max(x))\n",
        "    return np.log(e_x / e_x.sum())\n",
        "\n",
        "def NLLLoss(logs, targets):\n",
        "    out = logs[range(len(targets)), targets]\n",
        "    return -out.sum()/len(out)\n",
        "\n",
        "def log_softmax_crossentropy_with_logits(logits,target):\n",
        "\n",
        "    out = np.zeros_like(logits)\n",
        "    out[np.arange(len(logits)),target] = 1\n",
        "    \n",
        "    softmax = np.exp(logits) / np.exp(logits).sum(axis=-1,keepdims=True)\n",
        "    \n",
        "    return (- out + softmax) / logits.shape[0]\n",
        "\n",
        "def forward(context_idxs, theta):\n",
        "    m = embeddings[context_idxs].reshape(1, -1)\n",
        "    n = linear(m, theta)\n",
        "    o = log_softmax(n)    \n",
        "    return m, n, o\n",
        "\n",
        "def backward(preds, theta, target_idxs):\n",
        "    m, n, o = preds\n",
        "    \n",
        "    dlog = log_softmax_crossentropy_with_logits(n, target_idxs)\n",
        "    dw = m.T.dot(dlog)\n",
        "    \n",
        "    return dw\n",
        "def optimize(theta, grad, lr=0.03):\n",
        "    theta -= grad * lr\n",
        "    return theta\n",
        "\n",
        "#Genrate training data\n",
        "theta = np.random.uniform(-1, 1, (2 * context_size * embed_dim, vocab_size))\n",
        "\n",
        "\n",
        "epoch_losses = {}\n",
        "\n",
        "for epoch in range(80):\n",
        "\n",
        "    losses =  []\n",
        "\n",
        "    for context, target in data:\n",
        "        context_idxs = np.array([word_to_ix[w] for w in context])\n",
        "        preds = forward(context_idxs, theta)\n",
        "\n",
        "        target_idxs = np.array([word_to_ix[target]])\n",
        "        loss = NLLLoss(preds[-1], target_idxs)\n",
        "\n",
        "        losses.append(loss)\n",
        "\n",
        "        grad = backward(preds, theta, target_idxs)\n",
        "        theta = optimize(theta, grad, lr=0.03)  \n",
        "    epoch_losses[epoch] = losses\n",
        "\n",
        "\n",
        "ix = np.arange(0,80)\n",
        "fig = plt.figure()\n",
        "fig.suptitle('Epoch/Losses', fontsize=20)\n",
        "plt.plot(ix,[epoch_losses[i][0] for i in ix])\n",
        "plt.xlabel('Epochs', fontsize=12)\n",
        "plt.ylabel('Losses', fontsize=12)\n",
        "Output:\n",
        "Text(0, 0.5, 'Losses')\n",
        " \n",
        "\n",
        "def predict(words):\n",
        "    context_idxs = np.array([word_to_ix[w] for w in words])\n",
        "    preds = forward(context_idxs, theta)\n",
        "    word = ix_to_word[np.argmax(preds[-1])]\n",
        "    \n",
        "    return word\n",
        "\n",
        "# (['we', 'are', 'to', 'study'], 'about')\n",
        "predict(['we', 'are', 'to', 'study'])\n",
        "Output:\n",
        "About\n",
        "\n",
        "def accuracy():\n",
        "    wrong = 0\n",
        "\n",
        "    for context, target in data:\n",
        "        if(predict(context) != target):\n",
        "            wrong += 1\n",
        "            \n",
        "    return (1 - (wrong / len(data)))\n",
        "\n",
        "accuracy()\n",
        "Output:\n",
        "\n",
        "1.0\n",
        "\n",
        "predict(['processes', 'manipulate', 'things', 'study'])\n",
        "Output:\n"
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "#importing necessary libraries\n",
        "import tensorflow as tf\n",
        "from tensorflow import keras\n",
        "import pandas as pd\n",
        "import numpy as np\n",
        "import matplotlib.pyplot as plt\n",
        "import random\n",
        "%matplotlib inline\n",
        "#import dataset and split into train and test data\n",
        "mnist = tf.keras.datasets.mnist\n",
        "(x_train, y_train), (x_test, y_test) = mnist.load_data()\n",
        "Downloading data from https://storage.googleapis.com/tensorflow/tf-keras-datasets/mnist.npz\n",
        "11490434/11490434 [==============================] - 0s 0us/step\n",
        "#to see length of training dataset\n",
        "len(x_train)\n",
        "60000\n",
        "##to see length of testing dataset\n",
        "len(x_test)\n",
        "10000\n",
        "#shape of training dataset  60,000 images having 28*28 size\n",
        "x_train.shape\n",
        "(60000, 28, 28)\n",
        "#shape of testing dataset  10,000 images having 28*28 size\n",
        "x_test.shape\n",
        "(10000, 28, 28)\n",
        "x_train[0]\n",
        "array([[  0,   0,   0,   0,   0,   0,   0,   0,   0,   0,   0,   0,   0,\n",
        "          0,   0,   0,   0,   0,   0,   0,   0,   0,   0,   0,   0,   0,\n",
        "          0,   0],\n",
        "       [  0,   0,   0,   0,   0,   0,   0,   0,   0,   0,   0,   0,   0,\n",
        "          0,   0,   0,   0,   0,   0,   0,   0,   0,   0,   0,   0,   0,\n",
        "          0,   0],\n",
        "       [  0,   0,   0,   0,   0,   0,   0,   0,   0,   0,   0,   0,   0,\n",
        "          0,   0,   0,   0,   0,   0,   0,   0,   0,   0,   0,   0,   0,\n",
        "          0,   0],\n",
        "       [  0,   0,   0,   0,   0,   0,   0,   0,   0,   0,   0,   0,   0,\n",
        "          0,   0,   0,   0,   0,   0,   0,   0,   0,   0,   0,   0,   0,\n",
        "          0,   0],\n",
        "       [  0,   0,   0,   0,   0,   0,   0,   0,   0,   0,   0,   0,   0,\n",
        "          0,   0,   0,   0,   0,   0,   0,   0,   0,   0,   0,   0,   0,\n",
        "          0,   0],\n",
        "       [  0,   0,   0,   0,   0,   0,   0,   0,   0,   0,   0,   0,   3,\n",
        "         18,  18,  18, 126, 136, 175,  26, 166, 255, 247, 127,   0,   0,\n",
        "          0,   0],\n",
        "       [  0,   0,   0,   0,   0,   0,   0,   0,  30,  36,  94, 154, 170,\n",
        "        253, 253, 253, 253, 253, 225, 172, 253, 242, 195,  64,   0,   0,\n",
        "          0,   0],\n",
        "       [  0,   0,   0,   0,   0,   0,   0,  49, 238, 253, 253, 253, 253,\n",
        "        253, 253, 253, 253, 251,  93,  82,  82,  56,  39,   0,   0,   0,\n",
        "          0,   0],\n",
        "       [  0,   0,   0,   0,   0,   0,   0,  18, 219, 253, 253, 253, 253,\n",
        "        253, 198, 182, 247, 241,   0,   0,   0,   0,   0,   0,   0,   0,\n",
        "          0,   0],\n",
        "       [  0,   0,   0,   0,   0,   0,   0,   0,  80, 156, 107, 253, 253,\n",
        "        205,  11,   0,  43, 154,   0,   0,   0,   0,   0,   0,   0,   0,\n",
        "          0,   0],\n",
        "       [  0,   0,   0,   0,   0,   0,   0,   0,   0,  14,   1, 154, 253,\n",
        "         90,   0,   0,   0,   0,   0,   0,   0,   0,   0,   0,   0,   0,\n",
        "          0,   0],\n",
        "       [  0,   0,   0,   0,   0,   0,   0,   0,   0,   0,   0, 139, 253,\n",
        "        190,   2,   0,   0,   0,   0,   0,   0,   0,   0,   0,   0,   0,\n",
        "          0,   0],\n",
        "       [  0,   0,   0,   0,   0,   0,   0,   0,   0,   0,   0,  11, 190,\n",
        "        253,  70,   0,   0,   0,   0,   0,   0,   0,   0,   0,   0,   0,\n",
        "          0,   0],\n",
        "       [  0,   0,   0,   0,   0,   0,   0,   0,   0,   0,   0,   0,  35,\n",
        "        241, 225, 160, 108,   1,   0,   0,   0,   0,   0,   0,   0,   0,\n",
        "          0,   0],\n",
        "       [  0,   0,   0,   0,   0,   0,   0,   0,   0,   0,   0,   0,   0,\n",
        "         81, 240, 253, 253, 119,  25,   0,   0,   0,   0,   0,   0,   0,\n",
        "          0,   0],\n",
        "       [  0,   0,   0,   0,   0,   0,   0,   0,   0,   0,   0,   0,   0,\n",
        "          0,  45, 186, 253, 253, 150,  27,   0,   0,   0,   0,   0,   0,\n",
        "          0,   0],\n",
        "       [  0,   0,   0,   0,   0,   0,   0,   0,   0,   0,   0,   0,   0,\n",
        "          0,   0,  16,  93, 252, 253, 187,   0,   0,   0,   0,   0,   0,\n",
        "          0,   0],\n",
        "       [  0,   0,   0,   0,   0,   0,   0,   0,   0,   0,   0,   0,   0,\n",
        "          0,   0,   0,   0, 249, 253, 249,  64,   0,   0,   0,   0,   0,\n",
        "          0,   0],\n",
        "       [  0,   0,   0,   0,   0,   0,   0,   0,   0,   0,   0,   0,   0,\n",
        "          0,  46, 130, 183, 253, 253, 207,   2,   0,   0,   0,   0,   0,\n",
        "          0,   0],\n",
        "       [  0,   0,   0,   0,   0,   0,   0,   0,   0,   0,   0,   0,  39,\n",
        "        148, 229, 253, 253, 253, 250, 182,   0,   0,   0,   0,   0,   0,\n",
        "          0,   0],\n",
        "       [  0,   0,   0,   0,   0,   0,   0,   0,   0,   0,  24, 114, 221,\n",
        "        253, 253, 253, 253, 201,  78,   0,   0,   0,   0,   0,   0,   0,\n",
        "          0,   0],\n",
        "       [  0,   0,   0,   0,   0,   0,   0,   0,  23,  66, 213, 253, 253,\n",
        "        253, 253, 198,  81,   2,   0,   0,   0,   0,   0,   0,   0,   0,\n",
        "          0,   0],\n",
        "       [  0,   0,   0,   0,   0,   0,  18, 171, 219, 253, 253, 253, 253,\n",
        "        195,  80,   9,   0,   0,   0,   0,   0,   0,   0,   0,   0,   0,\n",
        "          0,   0],\n",
        "       [  0,   0,   0,   0,  55, 172, 226, 253, 253, 253, 253, 244, 133,\n",
        "         11,   0,   0,   0,   0,   0,   0,   0,   0,   0,   0,   0,   0,\n",
        "          0,   0],\n",
        "       [  0,   0,   0,   0, 136, 253, 253, 253, 212, 135, 132,  16,   0,\n",
        "          0,   0,   0,   0,   0,   0,   0,   0,   0,   0,   0,   0,   0,\n",
        "          0,   0],\n",
        "       [  0,   0,   0,   0,   0,   0,   0,   0,   0,   0,   0,   0,   0,\n",
        "          0,   0,   0,   0,   0,   0,   0,   0,   0,   0,   0,   0,   0,\n",
        "          0,   0],\n",
        "       [  0,   0,   0,   0,   0,   0,   0,   0,   0,   0,   0,   0,   0,\n",
        "          0,   0,   0,   0,   0,   0,   0,   0,   0,   0,   0,   0,   0,\n",
        "          0,   0],\n",
        "       [  0,   0,   0,   0,   0,   0,   0,   0,   0,   0,   0,   0,   0,\n",
        "          0,   0,   0,   0,   0,   0,   0,   0,   0,   0,   0,   0,   0,\n",
        "          0,   0]], dtype=uint8)\n",
        "#to see how first image look\n",
        "plt.matshow(x_train[0])\n",
        "\n",
        "#normalize the images by scaling pixel intensities to the range 0,1\n",
        "\n",
        "x_train = x_train / 255\n",
        "x_test = x_test / 255\n",
        "x_train[0]\n",
        "array([[0.        , 0.        , 0.        , 0.        , 0.        ,\n",
        "        0.        , 0.        , 0.        , 0.        , 0.        ,\n",
        "        0.        , 0.        , 0.        , 0.        , 0.        ,\n",
        "        0.        , 0.        , 0.        , 0.        , 0.        ,\n",
        "        0.        , 0.        , 0.        , 0.        , 0.        ,\n",
        "        0.        , 0.        , 0.        ],\n",
        "       [0.        , 0.        , 0.        , 0.        , 0.        ,\n",
        "        0.        , 0.        , 0.        , 0.        , 0.        ,\n",
        "        0.        , 0.        , 0.        , 0.        , 0.        ,\n",
        "        0.        , 0.        , 0.        , 0.        , 0.        ,\n",
        "        0.        , 0.        , 0.        , 0.        , 0.        ,\n",
        "        0.        , 0.        , 0.        ],\n",
        "       [0.        , 0.        , 0.        , 0.        , 0.        ,\n",
        "        0.        , 0.        , 0.        , 0.        , 0.        ,\n",
        "        0.        , 0.        , 0.        , 0.        , 0.        ,\n",
        "        0.        , 0.        , 0.        , 0.        , 0.        ,\n",
        "        0.        , 0.        , 0.        , 0.        , 0.        ,\n",
        "        0.        , 0.        , 0.        ],\n",
        "       [0.        , 0.        , 0.        , 0.        , 0.        ,\n",
        "        0.        , 0.        , 0.        , 0.        , 0.        ,\n",
        "        0.        , 0.        , 0.        , 0.        , 0.        ,\n",
        "        0.        , 0.        , 0.        , 0.        , 0.        ,\n",
        "        0.        , 0.        , 0.        , 0.        , 0.        ,\n",
        "        0.        , 0.        , 0.        ],\n",
        "       [0.        , 0.        , 0.        , 0.        , 0.        ,\n",
        "        0.        , 0.        , 0.        , 0.        , 0.        ,\n",
        "        0.        , 0.        , 0.        , 0.        , 0.        ,\n",
        "        0.        , 0.        , 0.        , 0.        , 0.        ,\n",
        "        0.        , 0.        , 0.        , 0.        , 0.        ,\n",
        "        0.        , 0.        , 0.        ],\n",
        "       [0.        , 0.        , 0.        , 0.        , 0.        ,\n",
        "        0.        , 0.        , 0.        , 0.        , 0.        ,\n",
        "        0.        , 0.        , 0.01176471, 0.07058824, 0.07058824,\n",
        "        0.07058824, 0.49411765, 0.53333333, 0.68627451, 0.10196078,\n",
        "        0.65098039, 1.        , 0.96862745, 0.49803922, 0.        ,\n",
        "        0.        , 0.        , 0.        ],\n",
        "       [0.        , 0.        , 0.        , 0.        , 0.        ,\n",
        "        0.        , 0.        , 0.        , 0.11764706, 0.14117647,\n",
        "        0.36862745, 0.60392157, 0.66666667, 0.99215686, 0.99215686,\n",
        "        0.99215686, 0.99215686, 0.99215686, 0.88235294, 0.6745098 ,\n",
        "        0.99215686, 0.94901961, 0.76470588, 0.25098039, 0.        ,\n",
        "        0.        , 0.        , 0.        ],\n",
        "       [0.        , 0.        , 0.        , 0.        , 0.        ,\n",
        "        0.        , 0.        , 0.19215686, 0.93333333, 0.99215686,\n",
        "        0.99215686, 0.99215686, 0.99215686, 0.99215686, 0.99215686,\n",
        "        0.99215686, 0.99215686, 0.98431373, 0.36470588, 0.32156863,\n",
        "        0.32156863, 0.21960784, 0.15294118, 0.        , 0.        ,\n",
        "        0.        , 0.        , 0.        ],\n",
        "       [0.        , 0.        , 0.        , 0.        , 0.        ,\n",
        "        0.        , 0.        , 0.07058824, 0.85882353, 0.99215686,\n",
        "        0.99215686, 0.99215686, 0.99215686, 0.99215686, 0.77647059,\n",
        "        0.71372549, 0.96862745, 0.94509804, 0.        , 0.        ,\n",
        "        0.        , 0.        , 0.        , 0.        , 0.        ,\n",
        "        0.        , 0.        , 0.        ],\n",
        "       [0.        , 0.        , 0.        , 0.        , 0.        ,\n",
        "        0.        , 0.        , 0.        , 0.31372549, 0.61176471,\n",
        "        0.41960784, 0.99215686, 0.99215686, 0.80392157, 0.04313725,\n",
        "        0.        , 0.16862745, 0.60392157, 0.        , 0.        ,\n",
        "        0.        , 0.        , 0.        , 0.        , 0.        ,\n",
        "        0.        , 0.        , 0.        ],\n",
        "       [0.        , 0.        , 0.        , 0.        , 0.        ,\n",
        "        0.        , 0.        , 0.        , 0.        , 0.05490196,\n",
        "        0.00392157, 0.60392157, 0.99215686, 0.35294118, 0.        ,\n",
        "        0.        , 0.        , 0.        , 0.        , 0.        ,\n",
        "        0.        , 0.        , 0.        , 0.        , 0.        ,\n",
        "        0.        , 0.        , 0.        ],\n",
        "       [0.        , 0.        , 0.        , 0.        , 0.        ,\n",
        "        0.        , 0.        , 0.        , 0.        , 0.        ,\n",
        "        0.        , 0.54509804, 0.99215686, 0.74509804, 0.00784314,\n",
        "        0.        , 0.        , 0.        , 0.        , 0.        ,\n",
        "        0.        , 0.        , 0.        , 0.        , 0.        ,\n",
        "        0.        , 0.        , 0.        ],\n",
        "       [0.        , 0.        , 0.        , 0.        , 0.        ,\n",
        "        0.        , 0.        , 0.        , 0.        , 0.        ,\n",
        "        0.        , 0.04313725, 0.74509804, 0.99215686, 0.2745098 ,\n",
        "        0.        , 0.        , 0.        , 0.        , 0.        ,\n",
        "        0.        , 0.        , 0.        , 0.        , 0.        ,\n",
        "        0.        , 0.        , 0.        ],\n",
        "       [0.        , 0.        , 0.        , 0.        , 0.        ,\n",
        "        0.        , 0.        , 0.        , 0.        , 0.        ,\n",
        "        0.        , 0.        , 0.1372549 , 0.94509804, 0.88235294,\n",
        "        0.62745098, 0.42352941, 0.00392157, 0.        , 0.        ,\n",
        "        0.        , 0.        , 0.        , 0.        , 0.        ,\n",
        "        0.        , 0.        , 0.        ],\n",
        "       [0.        , 0.        , 0.        , 0.        , 0.        ,\n",
        "        0.        , 0.        , 0.        , 0.        , 0.        ,\n",
        "        0.        , 0.        , 0.        , 0.31764706, 0.94117647,\n",
        "        0.99215686, 0.99215686, 0.46666667, 0.09803922, 0.        ,\n",
        "        0.        , 0.        , 0.        , 0.        , 0.        ,\n",
        "        0.        , 0.        , 0.        ],\n",
        "       [0.        , 0.        , 0.        , 0.        , 0.        ,\n",
        "        0.        , 0.        , 0.        , 0.        , 0.        ,\n",
        "        0.        , 0.        , 0.        , 0.        , 0.17647059,\n",
        "        0.72941176, 0.99215686, 0.99215686, 0.58823529, 0.10588235,\n",
        "        0.        , 0.        , 0.        , 0.        , 0.        ,\n",
        "        0.        , 0.        , 0.        ],\n",
        "       [0.        , 0.        , 0.        , 0.        , 0.        ,\n",
        "        0.        , 0.        , 0.        , 0.        , 0.        ,\n",
        "        0.        , 0.        , 0.        , 0.        , 0.        ,\n",
        "        0.0627451 , 0.36470588, 0.98823529, 0.99215686, 0.73333333,\n",
        "        0.        , 0.        , 0.        , 0.        , 0.        ,\n",
        "        0.        , 0.        , 0.        ],\n",
        "       [0.        , 0.        , 0.        , 0.        , 0.        ,\n",
        "        0.        , 0.        , 0.        , 0.        , 0.        ,\n",
        "        0.        , 0.        , 0.        , 0.        , 0.        ,\n",
        "        0.        , 0.        , 0.97647059, 0.99215686, 0.97647059,\n",
        "        0.25098039, 0.        , 0.        , 0.        , 0.        ,\n",
        "        0.        , 0.        , 0.        ],\n",
        "       [0.        , 0.        , 0.        , 0.        , 0.        ,\n",
        "        0.        , 0.        , 0.        , 0.        , 0.        ,\n",
        "        0.        , 0.        , 0.        , 0.        , 0.18039216,\n",
        "        0.50980392, 0.71764706, 0.99215686, 0.99215686, 0.81176471,\n",
        "        0.00784314, 0.        , 0.        , 0.        , 0.        ,\n",
        "        0.        , 0.        , 0.        ],\n",
        "       [0.        , 0.        , 0.        , 0.        , 0.        ,\n",
        "        0.        , 0.        , 0.        , 0.        , 0.        ,\n",
        "        0.        , 0.        , 0.15294118, 0.58039216, 0.89803922,\n",
        "        0.99215686, 0.99215686, 0.99215686, 0.98039216, 0.71372549,\n",
        "        0.        , 0.        , 0.        , 0.        , 0.        ,\n",
        "        0.        , 0.        , 0.        ],\n",
        "       [0.        , 0.        , 0.        , 0.        , 0.        ,\n",
        "        0.        , 0.        , 0.        , 0.        , 0.        ,\n",
        "        0.09411765, 0.44705882, 0.86666667, 0.99215686, 0.99215686,\n",
        "        0.99215686, 0.99215686, 0.78823529, 0.30588235, 0.        ,\n",
        "        0.        , 0.        , 0.        , 0.        , 0.        ,\n",
        "        0.        , 0.        , 0.        ],\n",
        "       [0.        , 0.        , 0.        , 0.        , 0.        ,\n",
        "        0.        , 0.        , 0.        , 0.09019608, 0.25882353,\n",
        "        0.83529412, 0.99215686, 0.99215686, 0.99215686, 0.99215686,\n",
        "        0.77647059, 0.31764706, 0.00784314, 0.        , 0.        ,\n",
        "        0.        , 0.        , 0.        , 0.        , 0.        ,\n",
        "        0.        , 0.        , 0.        ],\n",
        "       [0.        , 0.        , 0.        , 0.        , 0.        ,\n",
        "        0.        , 0.07058824, 0.67058824, 0.85882353, 0.99215686,\n",
        "        0.99215686, 0.99215686, 0.99215686, 0.76470588, 0.31372549,\n",
        "        0.03529412, 0.        , 0.        , 0.        , 0.        ,\n",
        "        0.        , 0.        , 0.        , 0.        , 0.        ,\n",
        "        0.        , 0.        , 0.        ],\n",
        "       [0.        , 0.        , 0.        , 0.        , 0.21568627,\n",
        "        0.6745098 , 0.88627451, 0.99215686, 0.99215686, 0.99215686,\n",
        "        0.99215686, 0.95686275, 0.52156863, 0.04313725, 0.        ,\n",
        "        0.        , 0.        , 0.        , 0.        , 0.        ,\n",
        "        0.        , 0.        , 0.        , 0.        , 0.        ,\n",
        "        0.        , 0.        , 0.        ],\n",
        "       [0.        , 0.        , 0.        , 0.        , 0.53333333,\n",
        "        0.99215686, 0.99215686, 0.99215686, 0.83137255, 0.52941176,\n",
        "        0.51764706, 0.0627451 , 0.        , 0.        , 0.        ,\n",
        "        0.        , 0.        , 0.        , 0.        , 0.        ,\n",
        "        0.        , 0.        , 0.        , 0.        , 0.        ,\n",
        "        0.        , 0.        , 0.        ],\n",
        "       [0.        , 0.        , 0.        , 0.        , 0.        ,\n",
        "        0.        , 0.        , 0.        , 0.        , 0.        ,\n",
        "        0.        , 0.        , 0.        , 0.        , 0.        ,\n",
        "        0.        , 0.        , 0.        , 0.        , 0.        ,\n",
        "        0.        , 0.        , 0.        , 0.        , 0.        ,\n",
        "        0.        , 0.        , 0.        ],\n",
        "       [0.        , 0.        , 0.        , 0.        , 0.        ,\n",
        "        0.        , 0.        , 0.        , 0.        , 0.        ,\n",
        "        0.        , 0.        , 0.        , 0.        , 0.        ,\n",
        "        0.        , 0.        , 0.        , 0.        , 0.        ,\n",
        "        0.        , 0.        , 0.        , 0.        , 0.        ,\n",
        "        0.        , 0.        , 0.        ],\n",
        "       [0.        , 0.        , 0.        , 0.        , 0.        ,\n",
        "        0.        , 0.        , 0.        , 0.        , 0.        ,\n",
        "        0.        , 0.        , 0.        , 0.        , 0.        ,\n",
        "        0.        , 0.        , 0.        , 0.        , 0.        ,\n",
        "        0.        , 0.        , 0.        , 0.        , 0.        ,\n",
        "        0.        , 0.        , 0.        ]])\n",
        "Define the network architecture using Keras\n",
        "Creating the model The ReLU function is one of the most popular activation functions. It stands for “rectified linear unit”. Mathematically this function is defined as: y = max(0,x)The ReLU function returns “0” if the input is negative and is linear if the input is positive.\n",
        "\n",
        "The softmax function is another activation function. It changes input values into values that reach from 0 to 1.\n",
        "\n",
        "model = keras.Sequential([\n",
        "    keras.layers.Flatten(input_shape=(28, 28)),\n",
        "    keras.layers.Dense(128, activation='relu'),\n",
        "    keras.layers.Dense(10, activation='softmax')\n",
        "])\n",
        "model.summary()\n",
        "Model: \"sequential_2\"\n",
        "_________________________________________________________________\n",
        " Layer (type)                Output Shape              Param #   \n",
        "=================================================================\n",
        " flatten_2 (Flatten)         (None, 784)               0         \n",
        "                                                                 \n",
        " dense_4 (Dense)             (None, 128)               100480    \n",
        "                                                                 \n",
        " dense_5 (Dense)             (None, 10)                1290      \n",
        "                                                                 \n",
        "=================================================================\n",
        "Total params: 101,770\n",
        "Trainable params: 101,770\n",
        "Non-trainable params: 0\n",
        "_________________________________________________________________\n",
        "Compile the model\n",
        "\n",
        "model.compile(optimizer='sgd',\n",
        "              loss='sparse_categorical_crossentropy',\n",
        "              metrics=['accuracy'])\n",
        "Train the model\n",
        "\n",
        "history=model.fit(x_train, y_train,validation_data=(x_test,y_test),epochs=10)\n",
        "Epoch 1/10\n",
        "1875/1875 [==============================] - 8s 4ms/step - loss: 0.6661 - accuracy: 0.8312 - val_loss: 0.3627 - val_accuracy: 0.9026\n",
        "Epoch 2/10\n",
        "1875/1875 [==============================] - 7s 3ms/step - loss: 0.3406 - accuracy: 0.9054 - val_loss: 0.2941 - val_accuracy: 0.9195\n",
        "Epoch 3/10\n",
        "1875/1875 [==============================] - 7s 4ms/step - loss: 0.2911 - accuracy: 0.9187 - val_loss: 0.2622 - val_accuracy: 0.9285\n",
        "Epoch 4/10\n",
        "1875/1875 [==============================] - 8s 4ms/step - loss: 0.2600 - accuracy: 0.9272 - val_loss: 0.2377 - val_accuracy: 0.9333\n",
        "Epoch 5/10\n",
        "1875/1875 [==============================] - 6s 3ms/step - loss: 0.2367 - accuracy: 0.9333 - val_loss: 0.2194 - val_accuracy: 0.9396\n",
        "Epoch 6/10\n",
        "1875/1875 [==============================] - 6s 3ms/step - loss: 0.2174 - accuracy: 0.9391 - val_loss: 0.2029 - val_accuracy: 0.9415\n",
        "Epoch 7/10\n",
        "1875/1875 [==============================] - 7s 4ms/step - loss: 0.2014 - accuracy: 0.9431 - val_loss: 0.1920 - val_accuracy: 0.9459\n",
        "Epoch 8/10\n",
        "1875/1875 [==============================] - 8s 4ms/step - loss: 0.1878 - accuracy: 0.9479 - val_loss: 0.1799 - val_accuracy: 0.9500\n",
        "Epoch 9/10\n",
        "1875/1875 [==============================] - 7s 3ms/step - loss: 0.1758 - accuracy: 0.9507 - val_loss: 0.1688 - val_accuracy: 0.9531\n",
        "Epoch 10/10\n",
        "1875/1875 [==============================] - 6s 3ms/step - loss: 0.1653 - accuracy: 0.9532 - val_loss: 0.1641 - val_accuracy: 0.9536\n",
        "Evaluate the model\n",
        "\n",
        "test_loss,test_acc=model.evaluate(x_test,y_test)\n",
        "print(\"Loss=%.3f\" %test_loss)\n",
        "print(\"Accuracy=%.3f\" %test_acc)\n",
        "---------------------------------------------------------------------------\n",
        "RuntimeError                              Traceback (most recent call last)\n",
        " in \n",
        "----> 1 test_loss,test_acc=model.evaluate(x_test,y_test)\n",
        "      2 print(\"Loss=%.3f\" %test_loss)\n",
        "      3 print(\"Accuracy=%.3f\" %test_acc)\n",
        "\n",
        "/usr/local/lib/python3.7/dist-packages/keras/utils/traceback_utils.py in error_handler(*args, **kwargs)\n",
        "     65     except Exception as e:  # pylint: disable=broad-except\n",
        "     66       filtered_tb = _process_traceback_frames(e.__traceback__)\n",
        "---> 67       raise e.with_traceback(filtered_tb) from None\n",
        "     68     finally:\n",
        "     69       del filtered_tb\n",
        "\n",
        "/usr/local/lib/python3.7/dist-packages/keras/engine/training.py in _assert_compile_was_called(self)\n",
        "   3158     # (i.e. whether the model is built and its inputs/outputs are set).\n",
        "   3159     if not self._is_compiled:\n",
        "-> 3160       raise RuntimeError('You must compile your model before '\n",
        "   3161                          'training/testing. '\n",
        "   3162                          'Use `model.compile(optimizer, loss)`.')\n",
        "\n",
        "RuntimeError: You must compile your model before training/testing. Use `model.compile(optimizer, loss)`.\n",
        "Making Prediction on New Data\n",
        "\n",
        "n=random.randint(0,9999)\n",
        "plt.imshow(x_test[n])\n",
        "plt.show()\n",
        "\n",
        "#we use predict() on new data\n",
        "predicted_value=model.predict(x_test)\n",
        "print(\"Handwritten number in the image is= %d\" %np.argmax(predicted_value[n]))\n",
        "313/313 [==============================] - 1s 2ms/step\n",
        "Handwritten number in the image is= 4\n",
        "Plot graph for Accuracy and Loss\n",
        "\n",
        "history.history??\n",
        "history.history.keys()\n",
        "plt.plot(history.history['accuracy'])\n",
        "plt.plot(history.history['val_accuracy'])\n",
        "plt.title('model accuracy')\n",
        "plt.ylabel('accuracy')\n",
        "plt.xlabel('epoch')\n",
        "plt.legend(['Train', 'Validation'], loc='upper left')\n",
        "plt.show()\n",
        "\n",
        "plt.plot(history.history['loss'])\n",
        "plt.plot(history.history['val_loss'])\n",
        "plt.title('model loss')\n",
        "plt.ylabel('loss')\n",
        "plt.xlabel('epoch')\n",
        "plt.legend(['Train', 'Validation'], loc='upper left')\n",
        "plt.show()\n",
        "\n",
        "plt.plot(history.history['accuracy'])\n",
        "plt.plot(history.history['val_accuracy'])\n",
        "plt.plot(history.history['loss'])\n",
        "plt.plot(history.history['val_loss'])\n",
        "plt.title('Training Loss and accuracy')\n",
        "plt.ylabel('accuracy/Loss')\n",
        "plt.xlabel('epoch')\n",
        "plt.legend(['accuracy', 'val_accuracy','loss','val_loss'])\n",
        "plt.show()\n",
        "\n",
        "Save the model\n",
        "\n",
        "keras_model_path='C:\\\\Users\\\\admin'\n",
        "model.save(keras_model_path)\n",
        "#use the save model\n",
        "restored_keras_model = tf.keras.models.load_model(keras_model_path)\n",
        "---------------------------------------------------------------------------\n",
        "FileNotFoundError                         Traceback (most recent call last)\n",
        " in \n",
        "      1 #use the save model\n",
        "----> 2 restored_keras_model = tf.keras.models.load_model(keras_model_path)\n",
        "\n",
        "/usr/local/lib/python3.7/dist-packages/keras/utils/traceback_utils.py in error_handler(*args, **kwargs)\n",
        "     65     except Exception as e:  # pylint: disable=broad-except\n",
        "     66       filtered_tb = _process_traceback_frames(e.__traceback__)\n",
        "---> 67       raise e.with_traceback(filtered_tb) from None\n",
        "     68     finally:\n",
        "     69       del filtered_tb\n",
        "\n",
        "/usr/local/lib/python3.7/dist-packages/tensorflow/python/saved_model/load.py in load_partial(export_dir, filters, tags, options)\n",
        "    914       except errors.NotFoundError as err:\n",
        "    915         raise FileNotFoundError(\n",
        "--> 916             str(err) + \"\\n You may be trying to load on a different device \"\n",
        "    917             \"from the computational device. Consider setting the \"\n",
        "    918             \"`experimental_io_device` option in `tf.saved_model.LoadOptions` \"\n",
        "\n",
        "FileNotFoundError: Unsuccessful TensorSliceReader constructor: Failed to find any matching files for C:\\Users\\admin/variables/variables\n",
        " You may be trying to load on a different device from the computational device. Consider setting the `experimental_io_device` option in `tf.saved_model.LoadOptions` to the io_device such as '/job:localhost'."
      ],
      "metadata": {
        "id": "Bf0pXtFFrgUR"
      },
      "execution_count": null,
      "outputs": []
    }
  ]
}