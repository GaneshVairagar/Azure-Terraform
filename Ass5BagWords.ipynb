{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "provenance": [],
      "authorship_tag": "ABX9TyMtafrWOoYRAJrkPSWrqsGS",
      "include_colab_link": true
    },
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3"
    },
    "language_info": {
      "name": "python"
    }
  },
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "view-in-github",
        "colab_type": "text"
      },
      "source": [
        "<a href=\"https://colab.research.google.com/github/GaneshVairagar/Azure-Terraform/blob/main/Ass5BagWords.ipynb\" target=\"_parent\"><img src=\"https://colab.research.google.com/assets/colab-badge.svg\" alt=\"Open In Colab\"/></a>"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 1,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 130
        },
        "id": "gpKSZX5Iq1A6",
        "outputId": "97555044-e567-4435-cd3d-261e0348d40c"
      },
      "outputs": [
        {
          "output_type": "error",
          "ename": "SyntaxError",
          "evalue": "ignored",
          "traceback": [
            "\u001b[0;36m  File \u001b[0;32m\"<ipython-input-1-357ad841d862>\"\u001b[0;36m, line \u001b[0;32m45\u001b[0m\n\u001b[0;31m    Output:\u001b[0m\n\u001b[0m           ^\u001b[0m\n\u001b[0;31mSyntaxError\u001b[0m\u001b[0;31m:\u001b[0m invalid syntax\n"
          ]
        }
      ],
      "source": [
        "import matplotlib.pyplot as plt\n",
        "import seaborn as sns\n",
        "import matplotlib as mpl\n",
        "import matplotlib.pylab as pylab\n",
        "import numpy as np\n",
        "%matplotlib inline\n",
        "\n",
        "#Data Prepration\n",
        "import re\n",
        "\n",
        "sentences = \"\"\"We are about to study the idea of a computational process.\n",
        "Computational processes are abstract beings that inhabit computers.\n",
        "As they evolve, processes manipulate other abstract things called data.\n",
        "The evolution of a process is directed by a pattern of rules\n",
        "called a program. People create programs to direct processes. In effect,\n",
        "we conjure the spirits of the computer with our spells.\"\"\"\n",
        "\n",
        "# remove special characters\n",
        "sentences = re.sub('[^A-Za-z0-9]+', ' ', sentences)\n",
        "\n",
        "# remove 1 letter words\n",
        "sentences = re.sub(r'(?:^| )\\w(?:$| )', ' ', sentences).strip()\n",
        "\n",
        "# lower all characters\n",
        "sentences = sentences.lower()\n",
        "\n",
        "words = sentences.split()\n",
        "vocab = set(words)\n",
        "\n",
        "vocab_size = len(vocab)\n",
        "embed_dim = 10\n",
        "context_size = 2\n",
        "\n",
        "word_to_ix = {word: i for i, word in enumerate(vocab)}\n",
        "ix_to_word = {i: word for i, word in enumerate(vocab)}\n",
        "\n",
        "# data - [(context), target]\n",
        "\n",
        "data = []\n",
        "for i in range(2, len(words) - 2):\n",
        "    context = [words[i - 2], words[i - 1], words[i + 1], words[i + 2]]\n",
        "    target = words[i]\n",
        "    data.append((context, target))\n",
        "print(data[:5])\n",
        "Output:\n",
        "[(['we', 'are', 'to', 'study'], 'about'), (['are', 'about', 'study', 'the'], 'to'), (['about', 'to', 'the', 'idea'], 'study'), (['to', 'study', 'idea', 'of'], 'the'), (['study', 'the', 'of', 'computational'], 'idea')]\n",
        "\n",
        "embeddings =  np.random.random_sample((vocab_size, embed_dim))\n",
        "\n",
        "def linear(m, theta):\n",
        "    w = theta\n",
        "    return m.dot(w)\n",
        "\n",
        "def log_softmax(x):\n",
        "    e_x = np.exp(x - np.max(x))\n",
        "    return np.log(e_x / e_x.sum())\n",
        "\n",
        "def NLLLoss(logs, targets):\n",
        "    out = logs[range(len(targets)), targets]\n",
        "    return -out.sum()/len(out)\n",
        "\n",
        "def log_softmax_crossentropy_with_logits(logits,target):\n",
        "\n",
        "    out = np.zeros_like(logits)\n",
        "    out[np.arange(len(logits)),target] = 1\n",
        "    \n",
        "    softmax = np.exp(logits) / np.exp(logits).sum(axis=-1,keepdims=True)\n",
        "    \n",
        "    return (- out + softmax) / logits.shape[0]\n",
        "\n",
        "def forward(context_idxs, theta):\n",
        "    m = embeddings[context_idxs].reshape(1, -1)\n",
        "    n = linear(m, theta)\n",
        "    o = log_softmax(n)    \n",
        "    return m, n, o\n",
        "\n",
        "def backward(preds, theta, target_idxs):\n",
        "    m, n, o = preds\n",
        "    \n",
        "    dlog = log_softmax_crossentropy_with_logits(n, target_idxs)\n",
        "    dw = m.T.dot(dlog)\n",
        "    \n",
        "    return dw\n",
        "def optimize(theta, grad, lr=0.03):\n",
        "    theta -= grad * lr\n",
        "    return theta\n",
        "\n",
        "#Genrate training data\n",
        "theta = np.random.uniform(-1, 1, (2 * context_size * embed_dim, vocab_size))\n",
        "\n",
        "\n",
        "epoch_losses = {}\n",
        "\n",
        "for epoch in range(80):\n",
        "\n",
        "    losses =  []\n",
        "\n",
        "    for context, target in data:\n",
        "        context_idxs = np.array([word_to_ix[w] for w in context])\n",
        "        preds = forward(context_idxs, theta)\n",
        "\n",
        "        target_idxs = np.array([word_to_ix[target]])\n",
        "        loss = NLLLoss(preds[-1], target_idxs)\n",
        "\n",
        "        losses.append(loss)\n",
        "\n",
        "        grad = backward(preds, theta, target_idxs)\n",
        "        theta = optimize(theta, grad, lr=0.03)  \n",
        "    epoch_losses[epoch] = losses\n",
        "\n",
        "\n",
        "ix = np.arange(0,80)\n",
        "fig = plt.figure()\n",
        "fig.suptitle('Epoch/Losses', fontsize=20)\n",
        "plt.plot(ix,[epoch_losses[i][0] for i in ix])\n",
        "plt.xlabel('Epochs', fontsize=12)\n",
        "plt.ylabel('Losses', fontsize=12)\n",
        "Output:\n",
        "Text(0, 0.5, 'Losses')\n",
        " \n",
        "\n",
        "def predict(words):\n",
        "    context_idxs = np.array([word_to_ix[w] for w in words])\n",
        "    preds = forward(context_idxs, theta)\n",
        "    word = ix_to_word[np.argmax(preds[-1])]\n",
        "    \n",
        "    return word\n",
        "\n",
        "# (['we', 'are', 'to', 'study'], 'about')\n",
        "predict(['we', 'are', 'to', 'study'])\n",
        "Output:\n",
        "About\n",
        "\n",
        "def accuracy():\n",
        "    wrong = 0\n",
        "\n",
        "    for context, target in data:\n",
        "        if(predict(context) != target):\n",
        "            wrong += 1\n",
        "            \n",
        "    return (1 - (wrong / len(data)))\n",
        "\n",
        "accuracy()\n",
        "Output:\n",
        "\n",
        "1.0\n",
        "\n",
        "predict(['processes', 'manipulate', 'things', 'study'])\n",
        "Output:\n"
      ]
    },
    {
      "cell_type": "code",
      "source": [],
      "metadata": {
        "id": "Bf0pXtFFrgUR"
      },
      "execution_count": null,
      "outputs": []
    }
  ]
}